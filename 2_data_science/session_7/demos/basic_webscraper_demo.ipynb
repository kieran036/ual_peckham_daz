{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check we can get the data from the page\n",
    "\n",
    "#here we use 'query' for the end of the url, this allows us to quickly change it\n",
    "query = 'AFC_Wimbledon'\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/' + query\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, features=\"html.parser\")\n",
    "print(soup)\n",
    "\n",
    "#this grabs us the html of the entire page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will check if the request was sucsessful. we want it to be 200, or at least start with a 2... anything else is a problem.\n",
    "\n",
    "print(page.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I want to find only the link on this page\n",
    "#first we create an array for the links\n",
    "\n",
    "links = []\n",
    "\n",
    "#we are looking for all of the <a> anchor tags.\n",
    "# we do this with a for loop, we use 'try' and 'except' as some of the anchors may not have an 'href'. we ignore these otherwise it could cause an error.\n",
    "\n",
    "for a in soup.find_all(\"a\"):\n",
    "    try:\n",
    "        links.append(a[\"href\"])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    #then another for loop to cycle though the array and print each link\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#many of the links are from outside wikipedia. in this case we only want internal links\n",
    "\n",
    "#we can then filter the array to only include links starting with /wiki/. so only internal links will show.\n",
    "\n",
    "filtered = [link for link in links if link.startswith('/wiki/')]\n",
    "\n",
    "for f in filtered:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are still a lot of links to stuff we dont want eg. pictures, help files ect. We can use ignore to filter them out.\n",
    "\n",
    "ignores = ['png', 'jpg', 'jpeg', 'isbn', 'svg', 'identifier', \\\n",
    "           'File', 'Special', 'Template', 'Mailto', 'Portal', \\\n",
    "           'Help', 'Category', 'Talk', 'Wikipedia', 'Main_Page']\n",
    "\n",
    "filtered = []\n",
    "\n",
    "#this line states only links that are to wiki pages are valid\n",
    "for link in links:\n",
    "    if link.startswith('/wiki/'):\n",
    "        valid = True\n",
    "\n",
    "# this line then makes all our ingnored links invalid\n",
    "        for ignore in ignores:\n",
    "            if ignore in link:\n",
    "                valid = False\n",
    "                break\n",
    "\n",
    "# if the link is valid we then add it to our 'filtered' array\n",
    "        if valid:\n",
    "            filtered.append(link)\n",
    "\n",
    "for f in filtered:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/AFC_Wimbledon\"\n",
    "\n",
    "# check the request was sucsessful (code 200)\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# here we find any element with the table tag, there are some of these we dont want on this page. \n",
    "# So we specify only tables using the \"wikitable\" class\n",
    "\n",
    "tabledata=soup.find('table',{'class':\"wikitable\"})\n",
    "\n",
    "#read the table data\n",
    "df=pd.read_html(str(tabledata))\n",
    "\n",
    "# convert list to pandas dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "print(df.head())\n",
    "\n",
    "#write the data to a .csv file\n",
    "df.to_csv('team_info.csv', sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
