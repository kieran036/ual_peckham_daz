{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API's in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an API? \n",
    "\n",
    "- API stands for **Application Programming Interface**.\n",
    "\n",
    "- A set of rules and protocols that allows different software applications to communicate with each other.\n",
    "\n",
    "- In the context of Python, APIs are used to request and receive data from web servers.\n",
    "\n",
    "- Specifically a **REST API** is a type of online service that we can access to receive (,send and delete) data.\n",
    "\n",
    "## Why use an API?\n",
    "\n",
    "- Efficient and structured way to access data\n",
    "\n",
    "- Less prone to breaking than web scraping - data is served in standard formats like JSON.\n",
    "  \n",
    "- Allows third-party interaction with other services e.g social media, weather apps etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Restaurant \n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcReqcxhovo8LV1uRKmOZ1TNnbCNrsCU5sJ7Cg&s\">\n",
    "\n",
    "Imagine you're in a restaurant...\n",
    "\n",
    "- You **(the client/customer)**\n",
    "- The waiter **(API)**\n",
    "- The kitchen **(Data Source)**\n",
    "\n",
    "Today you're hungry for data! \n",
    "\n",
    "To get your data food, your order needs to get to the kitchen, so like in any restaurant, you make a request via a waiter. \n",
    "\n",
    "Once your meal (data) has been prepared by the kitchen, the waiter will bring it to your table. \n",
    "\n",
    "This is a great analogy for understanding what API's do: they take requests for data, collect data from a server, and then deliver the response back to the user.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*re837GMp63hzDnB8tEiZKA.png\" style=\"width: 100%\">\n",
    "\n",
    "*If you wanted a comparison analogy for webscraping, that would be like going to a buffet and picking individual food items to make a meal.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Requests  \n",
    "\n",
    "-  HTTP Requests are what we will use to query an API for data. \n",
    "  \n",
    "-  We point to a specific URL, and give some extra information about the type of action we are doing (e.g. getting data, sending data, deleting data) \n",
    "  \n",
    "-  We can give some additional parameters (e.g. authorisation, specific queries etc...)\n",
    "  \n",
    "- We then get a response that tells us whether our request was successful and hopefully, the information we requested.\n",
    "\n",
    "#### There are common request types:\n",
    "\n",
    "-  **GET: Retrieve data from the server.**\n",
    "-  \n",
    "-  **POST: Submit data to the server.**\n",
    "-  \n",
    "-  **PUT: Update existing data on the server.**\n",
    "-  \n",
    "-  **DELETE: Remove data from the server.**\n",
    "\n",
    "\n",
    "We'll be focusing on **GET** requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Considerations of Working with API's \n",
    "\n",
    "### Authentication \n",
    "\n",
    "- Many APIs require an API key or OAuth for access.\n",
    "\n",
    "- Always keep your keys secure and **never hard-code them in your scripts!**\n",
    "\n",
    "### Rate Limiting \n",
    "\n",
    "- APIs often have rate limits to control the number of requests you can make.\n",
    "\n",
    "### Data Formats \n",
    "\n",
    "- JSON is the most common format for API responses.\n",
    "\n",
    "\n",
    "### HTTP Request Status Codes\n",
    "\n",
    "- ```200``` Successful response! \n",
    "- ```400``` Client Error - there is something wrong your side, check authorisation and request.\n",
    "- ```500``` Server Error - there is something wrong with the server, try again later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data from API's with python for Data Science\n",
    "<img src=\"https://cdn.sanity.io/images/oaglaatp/production/6fdc068d96ae659dac4248b1336fa9ce79b90cef-850x254.png?w=675&h=202&fit=crop\" style=\"width: 100%\">\n",
    "\n",
    "When collecting data from API's for data science purposes we'll be following this process: \n",
    "1. Review the API documentation \n",
    "   \n",
    "2. Request data from the API.\n",
    "   \n",
    "3. Analyse the JSON response and select the data we want. \n",
    "   \n",
    "4. Store the data in a pandas data frame for cleaning and analysis. \n",
    "   \n",
    "We'll also look at exporting this as CSV and Excel files. \n",
    "\n",
    "## Python libraries \n",
    "\n",
    "To make HTTP requests we'll be using the ```requests``` library. \n",
    "\n",
    "https://realpython.com/python-requests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: YouTube API\n",
    "\n",
    "Let's have a look at a code example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import api_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'api_keys' has no attribute 'YOUTUBE_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mapi_keys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mYOUTUBE_KEY\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'api_keys' has no attribute 'YOUTUBE_KEY'"
     ]
    }
   ],
   "source": [
    "print(api_keys.YOUTUBE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'YOUTUBE_KEY' from 'api_keys' (/Users/Kieran/Documents/Projects/personal/ual_peckham_daz/data_science/session_10/lecture/api_keys.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapi_keys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOUTUBE_KEY\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'YOUTUBE_KEY' from 'api_keys' (/Users/Kieran/Documents/Projects/personal/ual_peckham_daz/data_science/session_10/lecture/api_keys.py)"
     ]
    }
   ],
   "source": [
    "from api_keys import YOUTUBE_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'current_user_url': 'https://api.github.com/user',\n",
       " 'current_user_authorizations_html_url': 'https://github.com/settings/connections/applications{/client_id}',\n",
       " 'authorizations_url': 'https://api.github.com/authorizations',\n",
       " 'code_search_url': 'https://api.github.com/search/code?q={query}{&page,per_page,sort,order}',\n",
       " 'commit_search_url': 'https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}',\n",
       " 'emails_url': 'https://api.github.com/user/emails',\n",
       " 'emojis_url': 'https://api.github.com/emojis',\n",
       " 'events_url': 'https://api.github.com/events',\n",
       " 'feeds_url': 'https://api.github.com/feeds',\n",
       " 'followers_url': 'https://api.github.com/user/followers',\n",
       " 'following_url': 'https://api.github.com/user/following{/target}',\n",
       " 'gists_url': 'https://api.github.com/gists{/gist_id}',\n",
       " 'hub_url': 'https://api.github.com/hub',\n",
       " 'issue_search_url': 'https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}',\n",
       " 'issues_url': 'https://api.github.com/issues',\n",
       " 'keys_url': 'https://api.github.com/user/keys',\n",
       " 'label_search_url': 'https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}',\n",
       " 'notifications_url': 'https://api.github.com/notifications',\n",
       " 'organization_url': 'https://api.github.com/orgs/{org}',\n",
       " 'organization_repositories_url': 'https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}',\n",
       " 'organization_teams_url': 'https://api.github.com/orgs/{org}/teams',\n",
       " 'public_gists_url': 'https://api.github.com/gists/public',\n",
       " 'rate_limit_url': 'https://api.github.com/rate_limit',\n",
       " 'repository_url': 'https://api.github.com/repos/{owner}/{repo}',\n",
       " 'repository_search_url': 'https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}',\n",
       " 'current_user_repositories_url': 'https://api.github.com/user/repos{?type,page,per_page,sort}',\n",
       " 'starred_url': 'https://api.github.com/user/starred{/owner}{/repo}',\n",
       " 'starred_gists_url': 'https://api.github.com/gists/starred',\n",
       " 'topic_search_url': 'https://api.github.com/search/topics?q={query}{&page,per_page}',\n",
       " 'user_url': 'https://api.github.com/users/{user}',\n",
       " 'user_organizations_url': 'https://api.github.com/user/orgs',\n",
       " 'user_repositories_url': 'https://api.github.com/users/{user}/repos{?type,page,per_page,sort}',\n",
       " 'user_search_url': 'https://api.github.com/search/users?q={query}{&page,per_page,sort,order}'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"https://api.github.com\")\n",
    "\n",
    "print(response.status_code)\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the YouTube API\n",
    "\n",
    "The hardest part of working with API's is figuring out the structure of the URL.\n",
    "\n",
    "We currently have the root URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.googleapis.com/youtube/v3/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the location of our data. Now we need to define what data we want to collect.\n",
    "\n",
    "In this example we want to get video ID and information about the video.\n",
    "\n",
    "To get this right we need to look at the YouTube API documentation:\n",
    "\n",
    "<img src=\"https://cdn.sanity.io/images/oaglaatp/production/0431c1685464b66074c67a6307449eb4825b7e28-919x665.png?w=675&h=488&fit=crop\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = YOUTUBE_KEY\n",
    "channel_id = \"UC8butISFwT-Wl7EV0hUK0BQ\" # freecodecamps channel id\n",
    "page_token = \"\"\n",
    "\n",
    "request_url = f\"{url}search?key={api_key}&channelId={channel_id}&part=snippet,id&order=date&maxResults=1000&pageToken={page_token}\"\n",
    "request_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are performing a search through the YouTube API. Everything to the right of ```?``` is the parameters which are each separated by ```&```. \n",
    "\n",
    "We've used a f string to combine variables into one string.\n",
    "\n",
    "- First is the base URL\n",
    "- Then our API key\n",
    "- Then the channel we want to search \n",
    "- Then part, where we specify we want snippet and id data. \n",
    "- Then order, which is by date.\n",
    "- Then, max results is set to 1000.\n",
    "- Final pageToken, which we'll look at later...\n",
    "\n",
    "It can be pretty frustrating trying build the url for your use case. So start from the base url and work your way to the data you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting data\n",
    "\n",
    "Now we've got our URL we can use it to GET the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(request_url)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a 200 status code, which means the request was successful! \n",
    "\n",
    "Now let's save this data in a JSON object and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = response.json()\n",
    "\n",
    "response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the Data \n",
    "Great! We've made an API call and collected data, but we don't need all of it.\n",
    "\n",
    "Looking at the JSON data we can see where each video starts and ends, and the key value pairs in each video.\n",
    "\n",
    "Let's isolate the first video in the data and some specific information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_video = response_json['items'][0]\n",
    "\n",
    "first_video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's get the video id, title, and upload date and store them in variables...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = first_video['id']['videoId']\n",
    "video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_title = first_video['snippet']['title']\n",
    "video_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_date = first_video['snippet']['publishedAt']\n",
    "upload_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can clean this up and split the date and time by the \"T\" (time) character\n",
    "upload_date = str(upload_date).split(\"T\")[0]\n",
    "upload_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = f'Video Title: {video_title}, Video ID: {video_id}, Upload Date: {upload_date}'\n",
    "video_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have the video id, title, and upload date for the first video in the response. Let's put this into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df = pd.DataFrame(columns=[\"Video Title\", \"Video ID\", \"Upload Date\"], data=[[video_title, video_id, upload_date]])\n",
    "video_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping through the collected JSON data\n",
    "Ok, we've managed to get data for one video and store it in a data frame. \n",
    "\n",
    "But for data analysis we need lot's of data to make comparisons, so let's make a data frame with all of the videos we got from the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in response_json['items']: # loop through the items in the response\n",
    "    if video['id']['kind'] == 'youtube#video': # check if the item is a video!\n",
    "        video_id = video['id']['videoId']  # get the video ID\n",
    "        video_title = video['snippet']['title'] # get the video title\n",
    "        upload_date = str(video['snippet']['publishedAt']).split(\"T\")[0] # get the upload date and split the time\n",
    "\n",
    "        video_df = pd.concat([video_df, pd.DataFrame(data=[[video_title, video_id, upload_date]], columns=[\"Video Title\", \"Video ID\", \"Upload Date\"])], ignore_index=True)\n",
    "\n",
    "video_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting more data, with the data we've collected... \n",
    "\n",
    "We've now got a dataframe of videos from the freecodecamp channel. But this information doesn't really give us much insight into the videos and how they've performed. \n",
    "\n",
    "We can use the video ID's we've collected to make an API call for each video to collect additional information. \n",
    "\n",
    "Let's collect the view, comments, likes, and dislikes count for each video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df = pd.DataFrame(columns=[\"Video Title\", \"Video ID\", \"Upload Date\", \"View Count\", \"Like Count\", \"Comment Count\"]) # empty the dataframe\n",
    "\n",
    "for video in response_json['items']:\n",
    "\n",
    "    if video['id']['kind'] == 'youtube#video':\n",
    "        # What we did before\n",
    "        video_id = video['id']['videoId']\n",
    "        video_title = video['snippet']['title']\n",
    "        upload_date = str(video['snippet']['publishedAt']).split(\"T\")[0]\n",
    "\n",
    "        # Make a request to get the video statistics using the video ID\n",
    "        url_video_stats = f\"{url}videos?key={api_key}&part=statistics&id={video_id}\"\n",
    "        response_video_stats = requests.get(url_video_stats)\n",
    "        response_video_stats_json = response_video_stats.json()['items'][0]['statistics']\n",
    "\n",
    "        # Get the view count, like count, and comment count, and convert them to integers!\n",
    "        view_count = int(response_video_stats_json['viewCount'])\n",
    "        like_count = int(response_video_stats_json['likeCount'])\n",
    "        comment_count = int(response_video_stats_json['commentCount'])\n",
    "\n",
    "        video_df = pd.concat([video_df, pd.DataFrame(data=[[video_title, video_id, upload_date, view_count, like_count, comment_count]],\n",
    "                                                     columns=[\"Video Title\", \"Video ID\", \"Upload Date\", \"View Count\", \"Like Count\", \"Comment Count\"])],\n",
    "                                                    ignore_index=True)\n",
    "\n",
    "video_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organising and plotting\n",
    "\n",
    "Nice! We've got all our data and put it in a data frame. \n",
    "\n",
    "Using skills you've acquired from the last few weeks we can oraganise and visualise this.\n",
    "\n",
    "First reorganise this by likes, with most popular videos appearing first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_video_df = video_df.groupby('Video Title')['Like Count'].sum().reset_index().sort_values(by='Like Count', ascending=False)\n",
    "sort_video_df.reset_index(drop=True, inplace=True)\n",
    "sort_video_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the top 10 most popular videos by likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sort_video_df['Video Title'][:10], sort_video_df['Like Count'][:10], color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Like Count\")\n",
    "plt.title(\"Top 10 Videos by Like Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Data\n",
    "\n",
    "This is what we've done: \n",
    "\n",
    "- read the API documentation \n",
    "- requested data from the api \n",
    "- select what we want with JSON \n",
    "- use the data to collect more\n",
    "- plot the data \n",
    "\n",
    "Now finally we can export this data as a CSV or Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_video_df.to_csv('sorted_videos.csv')\n",
    "\n",
    "video_df.to_excel('video_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at another example with <a href=\"examples/spotipy.ipynb\">Spotipy</a>, which is a standalone python library for interacting with the spotify API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this session, we covered the following topics related to APIs:\n",
    "\n",
    "- **Introduction to APIs**: Understanding what APIs are and their basic functionality.\n",
    "- **Making API Requests**: How to make simple API requests using HTTP with Python requests library.\n",
    "- **Handling API Responses**: Basic techniques for parsing and handling responses from APIs.\n",
    "- **Extracting Data from Requests**: Converting JSON data to pandas dataframes and plotting it. \n",
    "\n",
    "**Resources:**\n",
    "- Youtube: https://developers.google.com/youtube/v3/docs\n",
    "- Spotipy: https://spotipy.readthedocs.io/en/2.24.0/\n",
    "- Python Requests: https://www.w3schools.com/python/module_requests.asp\n",
    "- Pandas: https://www.w3schools.com/python/pandas/pandas_cleaning.asp \n",
    "- Matplotlib: https://matplotlib.org/\n",
    "- Python JSON: https://www.w3schools.com/python/python_json.asp\n",
    "\n",
    "**List of public APIs**: https://github.com/public-apis/public-apis\n",
    "\n",
    "**Credits**:\n",
    "\n",
    "This class was made with reference to https://git.arts.ac.uk/mtanska/23-24-bsc-2-big-data/blob/main/week04/week04_coding_demo1.ipynb by Louis McCallum, Terrance Broad, Polo Sologub, and https://www.kdnuggets.com/2021/09/python-apis-data-science-project.html by Nate Rosidi. \n",
    "\n",
    "# Before you start the labs!\n",
    "\n",
    "## Create an openweather account and get your API key.\n",
    "Sometimes it takes a while to update on their server, so do this *before* we have a break! \n",
    "<a href=\"https://docs.openweather.co.uk/appid\">Instructions Here</a>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
